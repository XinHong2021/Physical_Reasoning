{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baseline models\n",
        "* Regression\n",
        "* CLassification\n",
        "    * BlockStackNet4() googlenet-(64,6) - highest validation score 0.4863"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HWaiSBwEkz8V"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7vkeZCssFSz"
      },
      "source": [
        "## Baseline Models is defined\n",
        "gogglenet\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k25H8iyjtQzN"
      },
      "source": [
        "## The data set file path is defined here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vwhixF45mZlT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "'''\n",
        "In this function block, the funciton is defined to do\n",
        "1. def__init__() : get the image from the trainning data path\n",
        "2. def __len__() : get the size of hte dataset\n",
        "3. def __getitem__() : get the ralated image the correct dataset\n",
        "'''\n",
        "class BlockStackDataset(Dataset):\n",
        "  '''\n",
        "  Return the image and its label. If not access or not exist, raise error.\n",
        "  '''\n",
        "  def __init__(self, data_frame, img_dir, transform = None):\n",
        "    self.data_frame = data_frame\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    if not os.path.exists(self.img_dir):\n",
        "      raise ValueError(f\"File path {self.img_dir} not exisits!\")\n",
        "    if not os.access(self.img_dir, os.R_OK):\n",
        "      raise ValueError(f\"File path {self.img_dir} is not readable!\")\n",
        "  '''\n",
        "  Return the size of the dataset\n",
        "  '''\n",
        "  def __len__(self):\n",
        "    return len(self.data_frame)\n",
        "\n",
        "  '''\n",
        "  Read the image and according to its id, get the related data in the train.csv.\n",
        "\n",
        "  Then return the corresponding image , stable_height, instability_type dataset provided.\n",
        "  '''\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = os.path.join(self.img_dir, str(self.data_frame.iloc[idx, 0])) # the first column in the train == image name\n",
        "    image = Image.open(img_name + \".jpg\")\n",
        "    label = self.data_frame.iloc[idx,3] # stable_height/instability_type\n",
        "    # label = label + 1\n",
        "    instability_type = self.data_frame.iloc[idx,4] # instability_type\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label, instability_type\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>shapeset</th>\n",
              "      <th>type</th>\n",
              "      <th>total_height</th>\n",
              "      <th>instability_type</th>\n",
              "      <th>cam_angle</th>\n",
              "      <th>stable_height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>173</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>245</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>465</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>611</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id  shapeset  type  total_height  instability_type  cam_angle  \\\n",
              "0   54         2     1             3                 1          1   \n",
              "1  173         1     1             4                 1          2   \n",
              "2  245         1     1             4                 1          2   \n",
              "3  465         2     1             5                 0          1   \n",
              "4  611         2     1             3                 1          1   \n",
              "\n",
              "   stable_height  \n",
              "0              2  \n",
              "1              1  \n",
              "2              1  \n",
              "3              5  \n",
              "4              1  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('./COMP90086_2024_Project_train/train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GACK9K_uPnV"
      },
      "source": [
        "## Trainner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xrOPfPa4t0AD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix,classification_report\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime as datatime\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NfTZbRKSxh2C"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The function here helps to create the logs recording the experiment results.\n",
        "'''\n",
        "def create_log_dir():\n",
        "  current_time = datatime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # log_dir = f'/content/drive/MyDrive/CV final project/runs/experiment_{current_time}'\n",
        "  # solution_dir = f'/content/drive/MyDrive/CV final project/trained_models/experiment_{current_time}'\n",
        "\n",
        "  log_dir = f'./runs/experiment_{current_time}'\n",
        "  solution_dir = f'./runs/experiment_{current_time}'\n",
        "  if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "  if not os.path.exists(solution_dir):\n",
        "    os.makedirs(solution_dir)\n",
        "\n",
        "  return log_dir, solution_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GoogLeNet(\n",
            "  (conv1): BasicConv2d(\n",
            "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (conv2): BasicConv2d(\n",
            "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): BasicConv2d(\n",
            "    (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (inception3a): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception3b): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (inception4a): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception4b): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception4c): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception4d): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception4e): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (maxpool4): Identity()\n",
            "  (inception5a): Identity()\n",
            "  (inception5b): Identity()\n",
            "  (aux1): None\n",
            "  (aux2): None\n",
            "  (avgpool): Identity()\n",
            "  (dropout): Identity()\n",
            "  (fc): Identity()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "googlenet = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "googlenet.fc = nn.Identity()\n",
        "googlenet.dropout = nn.Identity()\n",
        "googlenet.avgpool = nn.Identity()\n",
        "googlenet.inception5b = nn.Identity()\n",
        "googlenet.inception5a = nn.Identity()\n",
        "googlenet.maxpool4 = nn.Identity()\n",
        "# 打印模型结构，找到最后一个卷积层\n",
        "print(googlenet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 163072])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 创建一个随机的输入 (batch_size=1, channels=3, height=224, width=224)\n",
        "inputs = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# 获取 GoogeLeNet 的特征图\n",
        "with torch.no_grad():\n",
        "    output = googlenet(inputs)\n",
        "\n",
        "# 打印输出的形状\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "7ox5LZ7Aage0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Define models: googlenet\n",
        "\n",
        "# class TunnedBlockStackNet8(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(TunnedBlockStackNet8, self).__init__()\n",
        "#         # load the pre-trained model: gogglenet\n",
        "#         self.googlenet = models.googlenet(weights = models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "\n",
        "#        #  num_ftrs = self.googlenet.fc.in_features\n",
        "#         self.googlenet.fc = nn.Identity()\n",
        "#         self.googlenet.dropout = nn.Identity()\n",
        "#         self.googlenet.avgpool = nn.Identity()\n",
        "#         self.googlenet.inception5b = nn.Identity()\n",
        "#         self.googlenet.inception5a = nn.Identity()\n",
        "#         self.googlenet.maxpool4 = nn.Identity()\n",
        "\n",
        "#         self.additional_conv_layers = nn.Sequential(\n",
        "#             # 第一卷积层: 输入通道1024，输出通道512，卷积核3x3，步幅1，padding 1\n",
        "#             nn.Conv2d(in_channels=832, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(512),  # 添加批量归一化\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2),  # 最大池化层，输出特征图减半\n",
        "\n",
        "#             # 第二卷积层: 输入通道512，输出通道256，卷积核3x3，步幅1，padding 1\n",
        "#             nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(256),  # 添加批量归一化\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)  # 最大池化层，输出特征图再减半\n",
        "#         )\n",
        "\n",
        "#         self.fc = nn.Sequential(\n",
        "#             # nn.Linear(256, 256),\n",
        "#             # nn.ReLU(),\n",
        "#             # nn.Dropout(0.25),\n",
        "#             nn.Linear(256, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.25),\n",
        "#             nn.Linear(128, 6)\n",
        "\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         x = self.googlenet(x)\n",
        "#         # 知道通道数 channels，假设这里为 1024\n",
        "#         channels = 832\n",
        "\n",
        "#         # 动态计算 height 和 width\n",
        "#         total_size = x.size(1)  # 获取展平后的总大小\n",
        "#         height_width = int((total_size // channels) ** 0.5)  # 计算 height 和 width\n",
        "\n",
        "#         # 将张量转换为 4D 张量 [batch_size, channels, height, width]\n",
        "#         x = x.view(x.size(0), channels, height_width, height_width)\n",
        "\n",
        "#      #   x = x.view(32,1024,7,7)\n",
        "#        # x = x.view(x.size(0), -1)  # 展平成 (batch_size, 1024 * 7 * 7)\n",
        "\n",
        "#         # 经过附加的卷积层\n",
        "#         x = self.additional_conv_layers(x)  # 输出尺寸为 (batch_size, 256, 7, 7)\n",
        "#         # 平铺卷积层输出，以输入到全连接层\n",
        "#         x = x.view(x.size(0), -1)  # \n",
        "#         print(x.shape)\n",
        "#         x = self.fc(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define models: googlenet\n",
        "\n",
        "class TunnedBlockStackNet8(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TunnedBlockStackNet8, self).__init__()\n",
        "        # load the pre-trained model: gogglenet\n",
        "        self.googlenet = models.googlenet(weights = models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        num_ftrs = self.googlenet.fc.in_features\n",
        "        self.googlenet.fc = nn.Identity()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, 6)\n",
        "\n",
        "        )\n",
        "    def forward(self, x):\n",
        "      x = self.googlenet(x)\n",
        "      x = self.fc(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassificationBlockStackTrainer1:\n",
        "  '''\n",
        "  The function here helps to initalize the parameters used in the models and pre-process the image\n",
        "  '''\n",
        "  def __init__(self, csv_file, img_dir, model, stratify_column='stable_height', test_size=0.2,\n",
        "                 batch_size=32, num_epochs=10, learning_rate=0.001 ,random_state=42):\n",
        "        self.csv_file = csv_file\n",
        "        self.img_dir = img_dir\n",
        "        self.stratify_column = stratify_column\n",
        "        self.test_size = test_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = model\n",
        "\n",
        "        # load the train dataset\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "\n",
        "        # split data into train and validation dataset\n",
        "        self.train_data, self.val_data, self.train_ids, self.valid_ids = self.split_dataset()\n",
        "\n",
        "        # pre-processing the images\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "\n",
        "              #ADD MORE TRANSFORM METHODS HERE\n",
        "\n",
        "            transforms.RandomHorizontalFlip(),## ADDDED\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)), ## ADDDED\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),## ADDDED\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.train_loader = self.create_dataloader(self.train_data, self.transform)\n",
        "        self.val_loader = self.create_dataloader(self.val_data, self.transform, shuffle=False)\n",
        "\n",
        "\n",
        "        # use the gpu\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # set the loss weight\n",
        "        '''\n",
        "        Changed some thing here : to see the results\n",
        "        1. add the class_weights to each class labels with different weights,\n",
        "           more lables occurece with lower weight, few label occurence with higher weight;\n",
        "           without the weights, all the labels will be treated equally.\n",
        "\n",
        "        2. used to apply L2 regularization (also called weight decay).\n",
        "        The primary purpose of weight decay is to prevent overfitting by penalizing large weights.\n",
        "         It adds a penalty to the loss function based on the size of the weights\n",
        "         helps it generalize better to unseen data.\n",
        "         Lnew = Lold + weight_decay * sum(weight^2)\n",
        "\n",
        "        3. add scheduler to Reduces the learning rate after every step_size epochs.\n",
        "          After every 4 epochs, the learning rate will be multiplied by gamma (0.1 here), reducing it by 90%.\n",
        "        '''\n",
        "        class_weights = torch.tensor([100/25 , 100/25, 100/20,  100/15, 100/10,  100/5], device= self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=class_weights) # CrossEntropy for multi categorical-label predication\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate , weight_decay=0.0001)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size = 15, gamma=0.1)\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function helps to split the data set into the training and validation dataset according to the\n",
        "  size pre-determined.\n",
        "  '''\n",
        "\n",
        "  def split_dataset(self):\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=self.test_size, random_state=42)\n",
        "    train_ids = []\n",
        "    valid_ids = []\n",
        "    for train_idx, val_idx in split.split(self.data_frame, self.data_frame[self.stratify_column]):\n",
        "      train_data = self.data_frame.iloc[train_idx]\n",
        "      val_data = self.data_frame.iloc[val_idx]\n",
        "      train_ids.append(train_idx)\n",
        "      valid_ids.append(val_idx)\n",
        "    print(f\"Train dataset size: {len(train_data)}\",\n",
        "       f\"Validation dataset size: {len(val_data)}\",\n",
        "       f\"length of train_ids{(len(train_ids))}\",\n",
        "       f\"length of valid_ids{(len(valid_ids))}\")\n",
        "    return train_data, val_data, train_ids, valid_ids\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function helps to loda the image\n",
        "  '''\n",
        "  def create_dataloader(self, data_frame, transform, shuffle=True):\n",
        "    dataset = BlockStackDataset(data_frame, self.img_dir, transform=transform) # transform 可以用来数据增强\n",
        "    return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "  def generate_classification_report(self, outputs, labels):\n",
        "    predicted = outputs # 'outputs' is already a numpy array after prediction\n",
        "    labels = labels\n",
        "    print(classification_report(labels, predicted, zero_division=0))\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_confusion_matrix(self, outputs, labels):\n",
        "    predicted = outputs\n",
        "    labels = labels\n",
        "    matrix = confusion_matrix(labels, predicted)\n",
        "    print(matrix)\n",
        "\n",
        "\n",
        "  def validate(self):\n",
        "    self.model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_labels = []  # add all the lables\n",
        "    all_predictions = []  # add all the prediction\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, _ in self.val_loader:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device).long()\n",
        "            labels = labels - 1\n",
        "            outputs = self.model(inputs)\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "            # collecting all the lables and predictions\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "            # calculate the loss\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # calculate the correct predication\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # calculate the accuracy rate\n",
        "    val_accuracy = correct_predictions / total_samples\n",
        "    self.generate_classification_report(np.array(all_predictions), np.array(all_labels))\n",
        "    self.calculate_confusion_matrix(np.array(all_predictions), np.array(all_labels))\n",
        "    return val_loss/len(self.val_loader),val_accuracy, all_labels, all_predictions\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function here is used as the main training function on the image by using the pre-definned models in\n",
        "  hte first model class.\n",
        "  '''\n",
        "\n",
        "  def train(self):\n",
        "      _, solution_dir = create_log_dir()\n",
        "\n",
        "      best_val_accuracy = 0.0\n",
        "      for epoch in range(self.num_epochs):\n",
        "          self.model.train()\n",
        "          running_loss = 0.0\n",
        "          running_accuracy = 0.0\n",
        "\n",
        "          # monitor the process\n",
        "          with tqdm(self.train_loader, unit=\"batch\") as tepoch:\n",
        "              tepoch.set_description(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "              for inputs, labels, _ in tepoch:\n",
        "                  inputs, labels = inputs.to(self.device), labels.to(self.device).long()\n",
        "                  labels = labels - 1\n",
        "\n",
        "                  #forward propagation\n",
        "                  self.optimizer.zero_grad()\n",
        "                  raw_outputs = self.model(inputs)\n",
        "                  loss = self.criterion(raw_outputs, labels)  # loss calculation\n",
        "\n",
        "                  loss.backward()  # backward propagation\n",
        "                  self.optimizer.step()\n",
        "\n",
        "                  #Loss calculating\n",
        "                  running_loss += loss.item()\n",
        "                  _, predicted = torch.max(raw_outputs, 1)\n",
        "                  accuracy = (predicted == labels).sum().item()/ labels.size(0)\n",
        "                  running_accuracy += accuracy\n",
        "                  tepoch.set_postfix(loss=running_loss / len(self.train_loader),\n",
        "                            accuracy=running_accuracy / len(self.train_loader))\n",
        "\n",
        "\n",
        "\n",
        "          self.scheduler.step()\n",
        "          print(self.scheduler.get_last_lr())\n",
        "\n",
        "\n",
        "          val_loss,val_accuracy, all_labels, all_predictions = self.validate()\n",
        "          print(f\"Epoch {epoch + 1}/{self.num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "          if val_accuracy > best_val_accuracy:\n",
        "              best_val_accuracy = val_accuracy\n",
        "              torch.save(self.model.state_dict(), f'{solution_dir}/best_model.pth')\n",
        "              current_time = datatime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "              print('Best model saved!',current_time)\n",
        "\n",
        "      print('Finished Training')\n",
        "      print(f'Best validation accuracy: {best_val_accuracy:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "R3ipvbupjF6v"
      },
      "outputs": [],
      "source": [
        "class ClassificationBlockStackTrainer2:\n",
        "  '''\n",
        "  The function here helps to initalize the parameters used in the models and pre-process the image\n",
        "  '''\n",
        "  def __init__(self, csv_file, img_dir, model, stratify_column='stable_height', test_size=0.2,\n",
        "                 batch_size=32, num_epochs=10, learning_rate=0.01 ,random_state=42):\n",
        "        self.csv_file = csv_file\n",
        "        self.img_dir = img_dir\n",
        "        self.stratify_column = stratify_column\n",
        "        self.test_size = test_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = model\n",
        "\n",
        "        # load the train dataset\n",
        "        data_frame = pd.read_csv(csv_file)\n",
        "\n",
        "        data_frame1 = data_frame[data_frame['instability_type']==2]\n",
        "        randome_instable2 = data_frame1.sample(n=50,random_state=42)\n",
        "\n",
        "        data_frame2 = data_frame[data_frame['instability_type']!=2]\n",
        "        data_frame = pd.concat([randome_instable2,data_frame2])\n",
        "        self.data_frame = data_frame\n",
        "\n",
        "        # split data into train and validation dataset\n",
        "        self.train_data, self.val_data, self.train_ids, self.valid_ids = self.split_dataset()\n",
        "\n",
        "        # pre-processing the images\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "\n",
        "              #ADD MORE TRANSFORM METHODS HERE\n",
        "\n",
        "            transforms.RandomHorizontalFlip(),## ADDDED\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)), ## ADDDED\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),## ADDDED\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.train_loader = self.create_dataloader(self.train_data, self.transform)\n",
        "        self.val_loader = self.create_dataloader(self.val_data, self.transform, shuffle=False)\n",
        "\n",
        "\n",
        "        # use the gpu\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # set the loss weight\n",
        "        '''\n",
        "        Changed some thing here : to see the results\n",
        "        1. add the class_weights to each class labels with different weights,\n",
        "           more lables occurece with lower weight, few label occurence with higher weight;\n",
        "           without the weights, all the labels will be treated equally.\n",
        "\n",
        "        2. used to apply L2 regularization (also called weight decay).\n",
        "        The primary purpose of weight decay is to prevent overfitting by penalizing large weights.\n",
        "         It adds a penalty to the loss function based on the size of the weights\n",
        "         helps it generalize better to unseen data.\n",
        "         Lnew = Lold + weight_decay * sum(weight^2)\n",
        "\n",
        "        3. add scheduler to Reduces the learning rate after every step_size epochs.\n",
        "          After every 4 epochs, the learning rate will be multiplied by gamma (0.1 here), reducing it by 90%.\n",
        "        '''\n",
        "        class_weights = torch.tensor([100/25 , 100/25, 100/20,  100/15, 100/10,  100/5], device= self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=class_weights) # CrossEntropy for multi categorical-label predication\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate , weight_decay=0.0001)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size = 15, gamma=0.1)\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function helps to split the data set into the training and validation dataset according to the\n",
        "  size pre-determined.\n",
        "  '''\n",
        "\n",
        "  def split_dataset(self):\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=self.test_size, random_state=42)\n",
        "    train_ids = []\n",
        "    valid_ids = []\n",
        "    for train_idx, val_idx in split.split(self.data_frame, self.data_frame[self.stratify_column]):\n",
        "      train_data = self.data_frame.iloc[train_idx]\n",
        "      val_data = self.data_frame.iloc[val_idx]\n",
        "      train_ids.append(train_idx)\n",
        "      valid_ids.append(val_idx)\n",
        "    print(f\"Train dataset size: {len(train_data)}\",\n",
        "       f\"Validation dataset size: {len(val_data)}\",\n",
        "       f\"length of train_ids{(len(train_ids))}\",\n",
        "       f\"length of valid_ids{(len(valid_ids))}\")\n",
        "    return train_data, val_data, train_ids, valid_ids\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function helps to loda the image\n",
        "  '''\n",
        "  def create_dataloader(self, data_frame, transform, shuffle=True):\n",
        "    dataset = BlockStackDataset(data_frame, self.img_dir, transform=transform) # transform 可以用来数据增强\n",
        "    return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "  def generate_classification_report(self, outputs, labels):\n",
        "    predicted = outputs # 'outputs' is already a numpy array after prediction\n",
        "    labels = labels\n",
        "    print(classification_report(labels, predicted, zero_division=0))\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_confusion_matrix(self, outputs, labels):\n",
        "    predicted = outputs\n",
        "    labels = labels\n",
        "    matrix = confusion_matrix(labels, predicted)\n",
        "    print(matrix)\n",
        "\n",
        "\n",
        "  def validate(self):\n",
        "    self.model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_labels = []  # add all the lables\n",
        "    all_predictions = []  # add all the prediction\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, _ in self.val_loader:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device).long()\n",
        "            labels = labels - 1\n",
        "            outputs = self.model(inputs)\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "            # collecting all the lables and predictions\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "            # calculate the loss\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # calculate the correct predication\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # calculate the accuracy rate\n",
        "    val_accuracy = correct_predictions / total_samples\n",
        "    self.generate_classification_report(np.array(all_predictions), np.array(all_labels))\n",
        "    self.calculate_confusion_matrix(np.array(all_predictions), np.array(all_labels))\n",
        "    return val_loss/len(self.val_loader),val_accuracy, all_labels, all_predictions\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function here is used as the main training function on the image by using the pre-definned models in\n",
        "  hte first model class.\n",
        "  '''\n",
        "\n",
        "  def train(self):\n",
        "      _, solution_dir = create_log_dir()\n",
        "\n",
        "      best_val_accuracy = 0.0\n",
        "      for epoch in range(self.num_epochs):\n",
        "          self.model.train()\n",
        "          running_loss = 0.0\n",
        "          running_accuracy = 0.0\n",
        "\n",
        "          # monitor the process\n",
        "          with tqdm(self.train_loader, unit=\"batch\") as tepoch:\n",
        "              tepoch.set_description(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "              for inputs, labels, _ in tepoch:\n",
        "                  inputs, labels = inputs.to(self.device), labels.to(self.device).long()\n",
        "                  labels = labels - 1\n",
        "\n",
        "                  #forward propagation\n",
        "                  self.optimizer.zero_grad()\n",
        "                  raw_outputs = self.model(inputs)\n",
        "                  loss = self.criterion(raw_outputs, labels)  # loss calculation\n",
        "\n",
        "                  loss.backward()  # backward propagation\n",
        "                  self.optimizer.step()\n",
        "\n",
        "                  #Loss calculating\n",
        "                  running_loss += loss.item()\n",
        "                  _, predicted = torch.max(raw_outputs, 1)\n",
        "                  accuracy = (predicted == labels).sum().item()/ labels.size(0)\n",
        "                  running_accuracy += accuracy\n",
        "                  tepoch.set_postfix(loss=running_loss / len(self.train_loader),\n",
        "                            accuracy=running_accuracy / len(self.train_loader))\n",
        "\n",
        "\n",
        "\n",
        "          self.scheduler.step()\n",
        "          print(self.scheduler.get_last_lr())\n",
        "\n",
        "\n",
        "          val_loss,val_accuracy, all_labels, all_predictions = self.validate()\n",
        "          print(f\"Epoch {epoch + 1}/{self.num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "          if val_accuracy > best_val_accuracy:\n",
        "              best_val_accuracy = val_accuracy\n",
        "              torch.save(self.model.state_dict(), f'{solution_dir}/best_model.pth')\n",
        "              print('Best model saved!')\n",
        "\n",
        "      print('Finished Training')\n",
        "      print(f'Best validation accuracy: {best_val_accuracy:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model 3\n",
        "* Predict total height, see the performance of instability type 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 6144 Validation dataset size: 1536 length of train_ids1 length of valid_ids1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/16: 100%|██████████| 192/192 [05:35<00:00,  1.74s/batch, accuracy=0.647, loss=0.818]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.79      0.99      0.88       149\n",
            "           2       0.84      0.74      0.79       222\n",
            "           3       0.84      0.70      0.76       304\n",
            "           4       0.73      0.57      0.64       395\n",
            "           5       0.74      0.94      0.83       466\n",
            "\n",
            "    accuracy                           0.77      1536\n",
            "   macro avg       0.79      0.79      0.78      1536\n",
            "weighted avg       0.78      0.77      0.77      1536\n",
            "\n",
            "[[147   2   0   0   0]\n",
            " [ 38 165  19   0   0]\n",
            " [  2  28 212  60   2]\n",
            " [  0   1  19 226 149]\n",
            " [  0   1   3  24 438]]\n",
            "Epoch 1/16, Validation Loss: 0.5337, Validation Accuracy: 0.7734\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/16: 100%|██████████| 192/192 [05:33<00:00,  1.74s/batch, accuracy=0.806, loss=0.488]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.98      0.81      0.88       149\n",
            "           2       0.82      0.89      0.86       222\n",
            "           3       0.88      0.78      0.82       304\n",
            "           4       0.79      0.67      0.72       395\n",
            "           5       0.79      0.96      0.87       466\n",
            "\n",
            "    accuracy                           0.82      1536\n",
            "   macro avg       0.85      0.82      0.83      1536\n",
            "weighted avg       0.83      0.82      0.82      1536\n",
            "\n",
            "[[120  29   0   0   0]\n",
            " [  3 198  21   0   0]\n",
            " [  0  13 236  55   0]\n",
            " [  0   0  11 265 119]\n",
            " [  0   0   1  17 448]]\n",
            "Epoch 2/16, Validation Loss: 0.4094, Validation Accuracy: 0.8249\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/16: 100%|██████████| 192/192 [05:30<00:00,  1.72s/batch, accuracy=0.843, loss=0.407]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.82      0.99      0.90       149\n",
            "           2       0.78      0.86      0.82       222\n",
            "           3       0.65      0.81      0.72       304\n",
            "           4       0.52      0.67      0.59       395\n",
            "           5       0.98      0.47      0.64       466\n",
            "\n",
            "    accuracy                           0.70      1536\n",
            "   macro avg       0.75      0.76      0.73      1536\n",
            "weighted avg       0.75      0.70      0.69      1536\n",
            "\n",
            "[[148   1   0   0   0]\n",
            " [ 32 190   0   0   0]\n",
            " [  1  51 247   5   0]\n",
            " [  0   2 125 263   5]\n",
            " [  0   0  10 235 221]]\n",
            "Epoch 3/16, Validation Loss: 0.7457, Validation Accuracy: 0.6960\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/16: 100%|██████████| 192/192 [05:30<00:00,  1.72s/batch, accuracy=0.854, loss=0.377]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.92      1.00      0.96       149\n",
            "           2       0.88      0.95      0.91       222\n",
            "           3       0.81      0.89      0.85       304\n",
            "           4       0.75      0.81      0.78       395\n",
            "           5       0.96      0.77      0.85       466\n",
            "\n",
            "    accuracy                           0.85      1536\n",
            "   macro avg       0.86      0.88      0.87      1536\n",
            "weighted avg       0.86      0.85      0.85      1536\n",
            "\n",
            "[[149   0   0   0   0]\n",
            " [ 12 210   0   0   0]\n",
            " [  1  28 271   4   0]\n",
            " [  0   1  60 320  14]\n",
            " [  0   0   4 104 358]]\n",
            "Epoch 4/16, Validation Loss: 0.3852, Validation Accuracy: 0.8516\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/16: 100%|██████████| 192/192 [05:30<00:00,  1.72s/batch, accuracy=0.881, loss=0.317]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.94      0.99      0.96       149\n",
            "           2       0.95      0.92      0.94       222\n",
            "           3       0.93      0.90      0.92       304\n",
            "           4       0.87      0.81      0.84       395\n",
            "           5       0.88      0.95      0.91       466\n",
            "\n",
            "    accuracy                           0.90      1536\n",
            "   macro avg       0.91      0.91      0.91      1536\n",
            "weighted avg       0.90      0.90      0.90      1536\n",
            "\n",
            "[[147   2   0   0   0]\n",
            " [  9 204   9   0   0]\n",
            " [  0   8 275  21   0]\n",
            " [  0   0  12 320  63]\n",
            " [  0   0   0  25 441]]\n",
            "Epoch 5/16, Validation Loss: 0.2432, Validation Accuracy: 0.9030\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/16: 100%|██████████| 192/192 [05:42<00:00,  1.78s/batch, accuracy=0.886, loss=0.297]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.92      0.98      0.95       149\n",
            "           2       0.88      0.93      0.91       222\n",
            "           3       0.86      0.91      0.89       304\n",
            "           4       0.86      0.81      0.84       395\n",
            "           5       0.93      0.90      0.91       466\n",
            "\n",
            "    accuracy                           0.89      1536\n",
            "   macro avg       0.89      0.91      0.90      1536\n",
            "weighted avg       0.89      0.89      0.89      1536\n",
            "\n",
            "[[146   3   0   0   0]\n",
            " [ 13 207   2   0   0]\n",
            " [  0  23 277   4   0]\n",
            " [  0   1  41 320  33]\n",
            " [  0   0   1  46 419]]\n",
            "Epoch 6/16, Validation Loss: 0.2820, Validation Accuracy: 0.8913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/16: 100%|██████████| 192/192 [05:32<00:00,  1.73s/batch, accuracy=0.889, loss=0.293]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.99      0.97       149\n",
            "           2       0.88      0.96      0.92       222\n",
            "           3       0.93      0.88      0.91       304\n",
            "           4       0.87      0.85      0.86       395\n",
            "           5       0.91      0.91      0.91       466\n",
            "\n",
            "    accuracy                           0.90      1536\n",
            "   macro avg       0.91      0.92      0.91      1536\n",
            "weighted avg       0.90      0.90      0.90      1536\n",
            "\n",
            "[[147   2   0   0   0]\n",
            " [  8 214   0   0   0]\n",
            " [  0  23 269  11   1]\n",
            " [  0   3  16 337  39]\n",
            " [  0   0   3  41 422]]\n",
            "Epoch 7/16, Validation Loss: 0.2592, Validation Accuracy: 0.9043\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/16: 100%|██████████| 192/192 [05:47<00:00,  1.81s/batch, accuracy=0.894, loss=0.281]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.98      0.98       149\n",
            "           2       0.88      0.97      0.92       222\n",
            "           3       0.89      0.91      0.90       304\n",
            "           4       0.91      0.79      0.85       395\n",
            "           5       0.90      0.93      0.91       466\n",
            "\n",
            "    accuracy                           0.90      1536\n",
            "   macro avg       0.91      0.92      0.91      1536\n",
            "weighted avg       0.90      0.90      0.90      1536\n",
            "\n",
            "[[146   3   0   0   0]\n",
            " [  4 216   2   0   0]\n",
            " [  0  26 276   1   1]\n",
            " [  0   1  31 314  49]\n",
            " [  0   0   2  31 433]]\n",
            "Epoch 8/16, Validation Loss: 0.2628, Validation Accuracy: 0.9017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/16: 100%|██████████| 192/192 [05:34<00:00,  1.74s/batch, accuracy=0.906, loss=0.255]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      1.00      0.95       149\n",
            "           2       0.81      0.93      0.86       222\n",
            "           3       0.81      0.83      0.82       304\n",
            "           4       0.74      0.83      0.78       395\n",
            "           5       0.97      0.75      0.85       466\n",
            "\n",
            "    accuracy                           0.84      1536\n",
            "   macro avg       0.85      0.87      0.85      1536\n",
            "weighted avg       0.85      0.84      0.84      1536\n",
            "\n",
            "[[149   0   0   0   0]\n",
            " [ 15 206   1   0   0]\n",
            " [  0  45 253   6   0]\n",
            " [  0   4  53 326  12]\n",
            " [  0   0   5 110 351]]\n",
            "Epoch 9/16, Validation Loss: 0.4250, Validation Accuracy: 0.8366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/16: 100%|██████████| 192/192 [05:34<00:00,  1.74s/batch, accuracy=0.895, loss=0.277]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.95      0.96       149\n",
            "           2       0.94      0.90      0.92       222\n",
            "           3       0.92      0.87      0.90       304\n",
            "           4       0.86      0.89      0.87       395\n",
            "           5       0.91      0.95      0.93       466\n",
            "\n",
            "    accuracy                           0.91      1536\n",
            "   macro avg       0.92      0.91      0.92      1536\n",
            "weighted avg       0.91      0.91      0.91      1536\n",
            "\n",
            "[[142   7   0   0   0]\n",
            " [  5 199  18   0   0]\n",
            " [  0   6 265  33   0]\n",
            " [  0   0   4 350  41]\n",
            " [  0   0   0  25 441]]\n",
            "Epoch 10/16, Validation Loss: 0.2334, Validation Accuracy: 0.9095\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/16: 100%|██████████| 192/192 [05:38<00:00,  1.76s/batch, accuracy=0.906, loss=0.244]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.99      0.97       149\n",
            "           2       0.91      0.94      0.92       222\n",
            "           3       0.93      0.87      0.90       304\n",
            "           4       0.87      0.59      0.71       395\n",
            "           5       0.76      0.97      0.85       466\n",
            "\n",
            "    accuracy                           0.85      1536\n",
            "   macro avg       0.88      0.87      0.87      1536\n",
            "weighted avg       0.86      0.85      0.85      1536\n",
            "\n",
            "[[147   2   0   0   0]\n",
            " [  8 209   5   0   0]\n",
            " [  0  18 264  22   0]\n",
            " [  0   1  14 235 145]\n",
            " [  0   0   1  12 453]]\n",
            "Epoch 11/16, Validation Loss: 0.3312, Validation Accuracy: 0.8516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/16: 100%|██████████| 192/192 [05:31<00:00,  1.73s/batch, accuracy=0.913, loss=0.23]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.97      0.97       149\n",
            "           2       0.91      0.97      0.94       222\n",
            "           3       0.93      0.93      0.93       304\n",
            "           4       0.90      0.90      0.90       395\n",
            "           5       0.96      0.93      0.94       466\n",
            "\n",
            "    accuracy                           0.93      1536\n",
            "   macro avg       0.94      0.94      0.94      1536\n",
            "weighted avg       0.93      0.93      0.93      1536\n",
            "\n",
            "[[144   5   0   0   0]\n",
            " [  4 216   2   0   0]\n",
            " [  0  15 284   5   0]\n",
            " [  0   1  19 357  18]\n",
            " [  0   0   0  34 432]]\n",
            "Epoch 12/16, Validation Loss: 0.1720, Validation Accuracy: 0.9329\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/16: 100%|██████████| 192/192 [05:32<00:00,  1.73s/batch, accuracy=0.913, loss=0.231]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.99      0.97       149\n",
            "           2       0.93      0.95      0.94       222\n",
            "           3       0.92      0.93      0.92       304\n",
            "           4       0.90      0.83      0.87       395\n",
            "           5       0.91      0.94      0.92       466\n",
            "\n",
            "    accuracy                           0.92      1536\n",
            "   macro avg       0.92      0.93      0.92      1536\n",
            "weighted avg       0.91      0.92      0.91      1536\n",
            "\n",
            "[[147   2   0   0   0]\n",
            " [  8 211   3   0   0]\n",
            " [  0  13 283   7   1]\n",
            " [  0   1  22 329  43]\n",
            " [  0   0   1  29 436]]\n",
            "Epoch 13/16, Validation Loss: 0.2148, Validation Accuracy: 0.9154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/16: 100%|██████████| 192/192 [05:35<00:00,  1.75s/batch, accuracy=0.91, loss=0.24]     \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.99      0.97       149\n",
            "           2       0.91      0.95      0.93       222\n",
            "           3       0.92      0.92      0.92       304\n",
            "           4       0.90      0.85      0.87       395\n",
            "           5       0.92      0.93      0.92       466\n",
            "\n",
            "    accuracy                           0.92      1536\n",
            "   macro avg       0.92      0.93      0.92      1536\n",
            "weighted avg       0.92      0.92      0.92      1536\n",
            "\n",
            "[[147   2   0   0   0]\n",
            " [  8 212   2   0   0]\n",
            " [  0  17 281   6   0]\n",
            " [  0   2  20 334  39]\n",
            " [  0   0   1  31 434]]\n",
            "Epoch 14/16, Validation Loss: 0.2033, Validation Accuracy: 0.9167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/16: 100%|██████████| 192/192 [05:33<00:00,  1.73s/batch, accuracy=0.914, loss=0.227]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.93      0.96       149\n",
            "           2       0.91      0.91      0.91       222\n",
            "           3       0.92      0.87      0.89       304\n",
            "           4       0.85      0.50      0.63       395\n",
            "           5       0.70      0.98      0.82       466\n",
            "\n",
            "    accuracy                           0.82      1536\n",
            "   macro avg       0.87      0.84      0.84      1536\n",
            "weighted avg       0.84      0.82      0.81      1536\n",
            "\n",
            "[[138  11   0   0   0]\n",
            " [  2 202  17   1   0]\n",
            " [  0   9 264  28   3]\n",
            " [  0   0   6 198 191]\n",
            " [  0   0   1   6 459]]\n",
            "Epoch 15/16, Validation Loss: 0.5960, Validation Accuracy: 0.8210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/16: 100%|██████████| 192/192 [05:33<00:00,  1.74s/batch, accuracy=0.937, loss=0.167]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.99      0.98       149\n",
            "           2       0.91      0.97      0.94       222\n",
            "           3       0.91      0.94      0.92       304\n",
            "           4       0.92      0.89      0.91       395\n",
            "           5       0.96      0.94      0.95       466\n",
            "\n",
            "    accuracy                           0.93      1536\n",
            "   macro avg       0.94      0.94      0.94      1536\n",
            "weighted avg       0.94      0.93      0.93      1536\n",
            "\n",
            "[[147   2   0   0   0]\n",
            " [  5 215   2   0   0]\n",
            " [  0  17 285   2   0]\n",
            " [  0   1  25 353  16]\n",
            " [  0   0   1  29 436]]\n",
            "Epoch 16/16, Validation Loss: 0.1758, Validation Accuracy: 0.9349\n",
            "Best model saved!\n",
            "Finished Training\n",
            "Best validation accuracy: 0.9349\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "if __name__ == \"__main__\":\n",
        "    model = TunnedBlockStackNet8()\n",
        "    trainer = ClassificationBlockStackTrainer1(\n",
        "        csv_file = './COMP90086_2024_Project_train/train.csv', ##\n",
        "        img_dir='./COMP90086_2024_Project_train/train', ##\n",
        "        model=model,\n",
        "        test_size=0.2, # used to control the size of data in split_dataset(self)\n",
        "        num_epochs=16,\n",
        "        batch_size=32\n",
        "        )\n",
        "    trainer.train()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.96      0.99      0.97       149\n",
            "           2       0.92      0.97      0.95       222\n",
            "           3       0.92      0.95      0.94       304\n",
            "           4       0.93      0.89      0.91       395\n",
            "           5       0.96      0.94      0.95       466\n",
            "\n",
            "    accuracy                           0.94      1536\n",
            "   macro avg       0.94      0.95      0.94      1536\n",
            "weighted avg       0.94      0.94      0.94      1536\n",
            "\n",
            "[[147   2   0   0   0]\n",
            " [  6 215   1   0   0]\n",
            " [  0  15 288   1   0]\n",
            " [  0   1  22 352  20]\n",
            " [  0   0   1  26 439]]\n"
          ]
        }
      ],
      "source": [
        "val_loss,val_accuracy, all_labels, all_predictions = trainer.validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "total_height\n",
              "6    2304\n",
              "5    1920\n",
              "4    1536\n",
              "3    1152\n",
              "2     768\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.total_height.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_29836/3244600169.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  valid_df['pred_total_height'] = [pred+1 for pred in all_predictions]\n",
            "/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_29836/3244600169.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  valid_df['pred_type'] = valid_df['pred_total_height'] == valid_df['total_height']\n",
            "/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_29836/3244600169.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  valid_df['tt_pred_comp_stable_gt'] = valid_df['pred_total_height'] == valid_df['stable_height']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>shapeset</th>\n",
              "      <th>type</th>\n",
              "      <th>total_height</th>\n",
              "      <th>instability_type</th>\n",
              "      <th>cam_angle</th>\n",
              "      <th>stable_height</th>\n",
              "      <th>pred_total_height</th>\n",
              "      <th>pred_type</th>\n",
              "      <th>tt_pred_comp_stable_gt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7334</th>\n",
              "      <td>956915</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3955</th>\n",
              "      <td>516709</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>77447</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>212770</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5645</th>\n",
              "      <td>745098</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  shapeset  type  total_height  instability_type  cam_angle  \\\n",
              "7334  956915         2     2             4                 2          1   \n",
              "3955  516709         1     2             5                 1          1   \n",
              "619    77447         1     2             5                 1          1   \n",
              "1594  212770         2     2             4                 2          1   \n",
              "5645  745098         2     2             6                 1          1   \n",
              "\n",
              "      stable_height  pred_total_height  pred_type  tt_pred_comp_stable_gt  \n",
              "7334              1                  4       True                   False  \n",
              "3955              3                  5       True                   False  \n",
              "619               3                  5       True                   False  \n",
              "1594              1                  4       True                   False  \n",
              "5645              4                  6       True                   False  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_df = trainer.val_data\n",
        "valid_df['pred_total_height'] = [pred+1 for pred in all_predictions]\n",
        "valid_df['pred_type'] = valid_df['pred_total_height'] == valid_df['total_height']\n",
        "valid_df['tt_pred_comp_stable_gt'] = valid_df['pred_total_height'] == valid_df['stable_height']\n",
        "valid_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Accuracy of prediction of total height\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "instability_type\n",
              "0    0.956640\n",
              "1    0.935650\n",
              "2    0.925641\n",
              "Name: pred_type, dtype: float64"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\" Accuracy of prediction of total height\")\n",
        "valid_df.groupby('instability_type')['pred_type'].sum()/valid_df.groupby('instability_type')['pred_type'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of prediction of total height based on stable height\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "instability_type\n",
              "0    0.956640\n",
              "1    0.011583\n",
              "2    0.010256\n",
              "Name: tt_pred_comp_stable_gt, dtype: float64"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Accuracy of prediction of total height based on stable height\")\n",
        "valid_df.groupby('instability_type')['tt_pred_comp_stable_gt'].sum()/valid_df.groupby('instability_type')['tt_pred_comp_stable_gt'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>shapeset</th>\n",
              "      <th>type</th>\n",
              "      <th>total_height</th>\n",
              "      <th>instability_type</th>\n",
              "      <th>cam_angle</th>\n",
              "      <th>stable_height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>173</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>245</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>465</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>611</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id  shapeset  type  total_height  instability_type  cam_angle  \\\n",
              "0   54         2     1             3                 1          1   \n",
              "1  173         1     1             4                 1          2   \n",
              "2  245         1     1             4                 1          2   \n",
              "3  465         2     1             5                 0          1   \n",
              "4  611         2     1             3                 1          1   \n",
              "\n",
              "   stable_height  \n",
              "0              2  \n",
              "1              1  \n",
              "2              1  \n",
              "3              5  \n",
              "4              1  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model 2\n",
        "* add learning rate to 0.001\n",
        "* Worse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 4648 Validation dataset size: 1162 length of train_ids1 length of valid_ids1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/16: 100%|██████████| 146/146 [04:09<00:00,  1.71s/batch, accuracy=0.226, loss=1.74]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.01]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.25      1.00      0.39       285\n",
            "           2       0.00      0.00      0.00       233\n",
            "           3       0.00      0.00      0.00       180\n",
            "           4       0.00      0.00      0.00       128\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.25      1162\n",
            "   macro avg       0.04      0.17      0.07      1162\n",
            "weighted avg       0.06      0.25      0.10      1162\n",
            "\n",
            "[[  0 259   0   0   0   0]\n",
            " [  0 285   0   0   0   0]\n",
            " [  0 233   0   0   0   0]\n",
            " [  0 180   0   0   0   0]\n",
            " [  0 128   0   0   0   0]\n",
            " [  0  77   0   0   0   0]]\n",
            "Epoch 1/16, Validation Loss: 1.7159, Validation Accuracy: 0.2453\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/16: 100%|██████████| 146/146 [04:08<00:00,  1.70s/batch, accuracy=0.238, loss=1.72]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.01]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.25      1.00      0.39       285\n",
            "           2       0.00      0.00      0.00       233\n",
            "           3       0.00      0.00      0.00       180\n",
            "           4       0.00      0.00      0.00       128\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.25      1162\n",
            "   macro avg       0.04      0.17      0.07      1162\n",
            "weighted avg       0.06      0.25      0.10      1162\n",
            "\n",
            "[[  0 259   0   0   0   0]\n",
            " [  0 285   0   0   0   0]\n",
            " [  0 233   0   0   0   0]\n",
            " [  0 180   0   0   0   0]\n",
            " [  0 128   0   0   0   0]\n",
            " [  0  77   0   0   0   0]]\n",
            "Epoch 2/16, Validation Loss: 1.7126, Validation Accuracy: 0.2453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/16: 100%|██████████| 146/146 [04:09<00:00,  1.71s/batch, accuracy=0.24, loss=1.72]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.01]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.25      1.00      0.39       285\n",
            "           2       0.00      0.00      0.00       233\n",
            "           3       0.00      0.00      0.00       180\n",
            "           4       0.00      0.00      0.00       128\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.25      1162\n",
            "   macro avg       0.04      0.17      0.07      1162\n",
            "weighted avg       0.06      0.25      0.10      1162\n",
            "\n",
            "[[  0 259   0   0   0   0]\n",
            " [  0 285   0   0   0   0]\n",
            " [  0 233   0   0   0   0]\n",
            " [  0 180   0   0   0   0]\n",
            " [  0 128   0   0   0   0]\n",
            " [  0  77   0   0   0   0]]\n",
            "Epoch 3/16, Validation Loss: 1.7126, Validation Accuracy: 0.2453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/16:  75%|███████▍  | 109/146 [03:06<01:03,  1.71s/batch, accuracy=0.184, loss=1.28]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[96], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TunnedBlockStackNet8()\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ClassificationBlockStackTrainer1(\n\u001b[1;32m      5\u001b[0m     csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./COMP90086_2024_Project_train/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     img_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./COMP90086_2024_Project_train/train\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 12\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "Cell \u001b[0;32mIn[95], line 175\u001b[0m, in \u001b[0;36mClassificationBlockStackTrainer1.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m#forward propagation\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 175\u001b[0m raw_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[1;32m    176\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(raw_outputs, labels)  \u001b[38;5;66;03m# loss calculation\u001b[39;00m\n\u001b[1;32m    178\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# backward propagation\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[92], line 23\u001b[0m, in \u001b[0;36mTunnedBlockStackNet8.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgooglenet(x)\n\u001b[1;32m     24\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[1;32m     25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/googlenet.py:174\u001b[0m, in \u001b[0;36mGoogLeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GoogLeNetOutputs:\n\u001b[1;32m    173\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_input(x)\n\u001b[0;32m--> 174\u001b[0m     x, aux1, aux2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(x)\n\u001b[1;32m    175\u001b[0m     aux_defined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_logits\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/googlenet.py:112\u001b[0m, in \u001b[0;36mGoogLeNet._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# N x 3 x 224 x 224\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# N x 64 x 112 x 112\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool1(x)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/googlenet.py:273\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 273\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m    274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(x, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# test\n",
        "if __name__ == \"__main__\":\n",
        "    model = TunnedBlockStackNet8()\n",
        "    trainer = ClassificationBlockStackTrainer1(\n",
        "        csv_file = './COMP90086_2024_Project_train/train.csv', ##\n",
        "        img_dir='./COMP90086_2024_Project_train/train', ##\n",
        "        model=model,\n",
        "        test_size=0.2, # used to control the size of data in split_dataset(self)\n",
        "        num_epochs=16,\n",
        "        batch_size=32\n",
        "        )\n",
        "    trainer.train()\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 4648 Validation dataset size: 1162 length of train_ids1 length of valid_ids1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/16: 100%|██████████| 146/146 [04:11<00:00,  1.72s/batch, accuracy=0.249, loss=1.62]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.07      0.01      0.02       259\n",
            "           1       0.50      0.22      0.31       285\n",
            "           2       0.29      0.39      0.33       233\n",
            "           3       0.25      0.22      0.24       180\n",
            "           4       0.18      0.73      0.29       128\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.25      1162\n",
            "   macro avg       0.22      0.26      0.20      1162\n",
            "weighted avg       0.26      0.25      0.21      1162\n",
            "\n",
            "[[ 3 49 72 38 97  0]\n",
            " [ 5 63 95 31 91  0]\n",
            " [ 1 10 90 41 91  0]\n",
            " [ 5  3 34 40 98  0]\n",
            " [10  0 16  8 94  0]\n",
            " [18  0  3  2 54  0]]\n",
            "Epoch 1/16, Validation Loss: 1.6103, Validation Accuracy: 0.2496\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/16: 100%|██████████| 146/146 [04:13<00:00,  1.73s/batch, accuracy=0.274, loss=1.57]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.49      0.34      0.40       285\n",
            "           2       0.30      0.45      0.36       233\n",
            "           3       0.24      0.38      0.29       180\n",
            "           4       0.19      0.26      0.22       128\n",
            "           5       0.30      0.57      0.39        77\n",
            "\n",
            "    accuracy                           0.30      1162\n",
            "   macro avg       0.25      0.33      0.28      1162\n",
            "weighted avg       0.26      0.30      0.26      1162\n",
            "\n",
            "[[  0  70  81  52  32  24]\n",
            " [  0  96  81  55  39  14]\n",
            " [  0  26 105  60  28  14]\n",
            " [  0   2  62  68  25  23]\n",
            " [  0   0  17  49  33  29]\n",
            " [  0   2   8   5  18  44]]\n",
            "Epoch 2/16, Validation Loss: 1.5226, Validation Accuracy: 0.2978\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/16: 100%|██████████| 146/146 [04:10<00:00,  1.72s/batch, accuracy=0.283, loss=1.56]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.12      0.15       259\n",
            "           1       0.42      0.44      0.43       285\n",
            "           2       0.31      0.30      0.31       233\n",
            "           3       0.23      0.28      0.25       180\n",
            "           4       0.22      0.43      0.29       128\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.29      1162\n",
            "   macro avg       0.23      0.26      0.24      1162\n",
            "weighted avg       0.27      0.29      0.27      1162\n",
            "\n",
            "[[ 32  92  53  41  41   0]\n",
            " [ 17 125  45  52  46   0]\n",
            " [ 27  56  71  44  35   0]\n",
            " [ 22  11  49  50  48   0]\n",
            " [ 29   8   9  27  55   0]\n",
            " [ 43   5   5   2  22   0]]\n",
            "Epoch 3/16, Validation Loss: 1.5500, Validation Accuracy: 0.2866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/16: 100%|██████████| 146/146 [04:11<00:00,  1.72s/batch, accuracy=0.289, loss=1.55]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.55      0.31      0.39       285\n",
            "           2       0.31      0.63      0.42       233\n",
            "           3       0.27      0.20      0.23       180\n",
            "           4       0.22      0.63      0.32       128\n",
            "           5       0.71      0.19      0.31        77\n",
            "\n",
            "    accuracy                           0.31      1162\n",
            "   macro avg       0.34      0.33      0.28      1162\n",
            "weighted avg       0.31      0.31      0.27      1162\n",
            "\n",
            "[[  0  60 100  29  67   3]\n",
            " [  0  87 115  22  60   1]\n",
            " [  0  12 147  26  47   1]\n",
            " [  0   0  75  36  68   1]\n",
            " [  0   0  28  19  81   0]\n",
            " [  0   0  10   2  50  15]]\n",
            "Epoch 4/16, Validation Loss: 1.5233, Validation Accuracy: 0.3150\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/16: 100%|██████████| 146/146 [04:10<00:00,  1.72s/batch, accuracy=0.291, loss=1.54]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.68      0.38       259\n",
            "           1       0.22      0.13      0.16       285\n",
            "           2       0.16      0.18      0.17       233\n",
            "           3       0.20      0.07      0.11       180\n",
            "           4       0.00      0.00      0.00       128\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.23      1162\n",
            "   macro avg       0.14      0.18      0.14      1162\n",
            "weighted avg       0.18      0.23      0.18      1162\n",
            "\n",
            "[[176  31  46   6   0   0]\n",
            " [197  36  49   3   0   0]\n",
            " [149  35  43   6   0   0]\n",
            " [ 95  31  41  13   0   0]\n",
            " [ 35  27  49  17   0   0]\n",
            " [  5   7  45  20   0   0]]\n",
            "Epoch 5/16, Validation Loss: 1.9383, Validation Accuracy: 0.2306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/16: 100%|██████████| 146/146 [04:13<00:00,  1.74s/batch, accuracy=0.296, loss=1.53]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.25      0.32       259\n",
            "           1       0.26      0.11      0.16       285\n",
            "           2       0.28      0.39      0.33       233\n",
            "           3       0.23      0.27      0.25       180\n",
            "           4       0.20      0.51      0.28       128\n",
            "           5       0.48      0.14      0.22        77\n",
            "\n",
            "    accuracy                           0.27      1162\n",
            "   macro avg       0.31      0.28      0.26      1162\n",
            "weighted avg       0.30      0.27      0.26      1162\n",
            "\n",
            "[[66 32 66 32 60  3]\n",
            " [82 32 71 40 58  2]\n",
            " [10 46 90 49 38  0]\n",
            " [ 1  4 64 48 60  3]\n",
            " [ 0  3 18 38 65  4]\n",
            " [ 0  4  9  4 49 11]]\n",
            "Epoch 6/16, Validation Loss: 1.5067, Validation Accuracy: 0.2685\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/16: 100%|██████████| 146/146 [04:12<00:00,  1.73s/batch, accuracy=0.314, loss=1.51]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.48      0.37      0.42       285\n",
            "           2       0.34      0.33      0.34       233\n",
            "           3       0.25      0.52      0.34       180\n",
            "           4       0.21      0.33      0.26       128\n",
            "           5       0.28      0.53      0.36        77\n",
            "\n",
            "    accuracy                           0.31      1162\n",
            "   macro avg       0.26      0.35      0.29      1162\n",
            "weighted avg       0.27      0.31      0.28      1162\n",
            "\n",
            "[[  0  78  48  75  30  28]\n",
            " [  0 106  63  58  32  26]\n",
            " [  0  30  77  73  40  13]\n",
            " [  0   3  30  93  30  24]\n",
            " [  0   4   3  62  42  17]\n",
            " [  0   0   3  11  22  41]]\n",
            "Epoch 7/16, Validation Loss: 1.5139, Validation Accuracy: 0.3090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/16: 100%|██████████| 146/146 [04:09<00:00,  1.71s/batch, accuracy=0.319, loss=1.51]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.44      0.28      0.34       285\n",
            "           2       0.31      0.58      0.40       233\n",
            "           3       0.31      0.16      0.21       180\n",
            "           4       0.22      0.62      0.33       128\n",
            "           5       0.28      0.35      0.31        77\n",
            "\n",
            "    accuracy                           0.30      1162\n",
            "   macro avg       0.26      0.33      0.27      1162\n",
            "weighted avg       0.26      0.30      0.25      1162\n",
            "\n",
            "[[  0  65 102  22  46  24]\n",
            " [  0  79 116  10  67  13]\n",
            " [  0  17 135  22  53   6]\n",
            " [  0   6  57  29  74  14]\n",
            " [  0   6  20  10  79  13]\n",
            " [  0   5   6   1  38  27]]\n",
            "Epoch 8/16, Validation Loss: 1.5389, Validation Accuracy: 0.3003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/16: 100%|██████████| 146/146 [04:11<00:00,  1.72s/batch, accuracy=0.327, loss=1.5]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.37      0.60      0.45       285\n",
            "           2       0.34      0.38      0.36       233\n",
            "           3       0.31      0.27      0.29       180\n",
            "           4       0.31      0.38      0.34       128\n",
            "           5       0.23      0.38      0.28        77\n",
            "\n",
            "    accuracy                           0.33      1162\n",
            "   macro avg       0.26      0.33      0.29      1162\n",
            "weighted avg       0.26      0.33      0.28      1162\n",
            "\n",
            "[[  0 122  61  29  14  33]\n",
            " [  1 170  55  15  24  20]\n",
            " [  0  82  89  32  21   9]\n",
            " [  0  39  45  48  28  20]\n",
            " [  0  27  10  26  48  17]\n",
            " [  0  23   1   3  21  29]]\n",
            "Epoch 9/16, Validation Loss: 1.5213, Validation Accuracy: 0.3305\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/16: 100%|██████████| 146/146 [04:08<00:00,  1.70s/batch, accuracy=0.327, loss=1.51]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.41      0.29      0.34       285\n",
            "           2       0.34      0.34      0.34       233\n",
            "           3       0.29      0.23      0.26       180\n",
            "           4       0.22      0.41      0.29       128\n",
            "           5       0.18      0.81      0.30        77\n",
            "\n",
            "    accuracy                           0.27      1162\n",
            "   macro avg       0.24      0.35      0.25      1162\n",
            "weighted avg       0.25      0.27      0.24      1162\n",
            "\n",
            "[[ 0 56 65 35 39 64]\n",
            " [ 0 83 77 28 41 56]\n",
            " [ 0 25 80 39 44 45]\n",
            " [ 0 14 14 42 54 56]\n",
            " [ 0 14  0  3 52 59]\n",
            " [ 0  9  0  0  6 62]]\n",
            "Epoch 10/16, Validation Loss: 1.5857, Validation Accuracy: 0.2745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/16: 100%|██████████| 146/146 [04:08<00:00,  1.70s/batch, accuracy=0.341, loss=1.49]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       259\n",
            "           1       0.33      0.65      0.44       285\n",
            "           2       0.31      0.39      0.34       233\n",
            "           3       0.22      0.36      0.27       180\n",
            "           4       0.11      0.02      0.04       128\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.29      1162\n",
            "   macro avg       0.16      0.24      0.18      1162\n",
            "weighted avg       0.19      0.29      0.22      1162\n",
            "\n",
            "[[  0 141  69  43   6   0]\n",
            " [  0 184  63  37   1   0]\n",
            " [  0 105  91  35   2   0]\n",
            " [  0  51  61  64   4   0]\n",
            " [  0  37  12  76   3   0]\n",
            " [  0  33   0  32  12   0]]\n",
            "Epoch 11/16, Validation Loss: 1.5371, Validation Accuracy: 0.2943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/16: 100%|██████████| 146/146 [04:08<00:00,  1.70s/batch, accuracy=0.336, loss=1.48]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.23      0.28       259\n",
            "           1       0.31      0.25      0.27       285\n",
            "           2       0.33      0.50      0.40       233\n",
            "           3       0.25      0.23      0.24       180\n",
            "           4       0.31      0.37      0.33       128\n",
            "           5       0.29      0.40      0.34        77\n",
            "\n",
            "    accuracy                           0.31      1162\n",
            "   macro avg       0.31      0.33      0.31      1162\n",
            "weighted avg       0.32      0.31      0.31      1162\n",
            "\n",
            "[[ 59  52  78  30  19  21]\n",
            " [ 83  70  73  26  16  17]\n",
            " [ 11  54 116  28  16   8]\n",
            " [  2  26  63  42  32  15]\n",
            " [  1  13  17  34  47  16]\n",
            " [  2  14   1   6  23  31]]\n",
            "Epoch 12/16, Validation Loss: 1.5316, Validation Accuracy: 0.3141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/16: 100%|██████████| 146/146 [04:10<00:00,  1.71s/batch, accuracy=0.35, loss=1.47]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.08      0.00      0.01       259\n",
            "           1       0.35      0.60      0.44       285\n",
            "           2       0.36      0.31      0.33       233\n",
            "           3       0.28      0.20      0.23       180\n",
            "           4       0.25      0.21      0.23       128\n",
            "           5       0.21      0.64      0.32        77\n",
            "\n",
            "    accuracy                           0.31      1162\n",
            "   macro avg       0.26      0.33      0.26      1162\n",
            "weighted avg       0.26      0.31      0.26      1162\n",
            "\n",
            "[[  1 136  43  23   9  47]\n",
            " [  3 171  44  17  13  37]\n",
            " [  3  89  72  34  16  19]\n",
            " [  2  42  34  36  35  31]\n",
            " [  1  31   5  18  27  46]\n",
            " [  3  16   0   2   7  49]]\n",
            "Epoch 13/16, Validation Loss: 1.4779, Validation Accuracy: 0.3064\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/16: 100%|██████████| 146/146 [04:09<00:00,  1.71s/batch, accuracy=0.351, loss=1.46]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.02      0.03       259\n",
            "           1       0.33      0.85      0.48       285\n",
            "           2       0.45      0.19      0.27       233\n",
            "           3       0.36      0.36      0.36       180\n",
            "           4       0.27      0.28      0.28       128\n",
            "           5       0.50      0.05      0.09        77\n",
            "\n",
            "    accuracy                           0.34      1162\n",
            "   macro avg       0.38      0.29      0.25      1162\n",
            "weighted avg       0.37      0.34      0.27      1162\n",
            "\n",
            "[[  4 191  17  29  17   1]\n",
            " [  2 242  13  16  10   2]\n",
            " [  4 146  45  30   8   0]\n",
            " [  1  70  22  64  23   0]\n",
            " [  0  55   2  34  36   1]\n",
            " [  1  29   0   5  38   4]]\n",
            "Epoch 14/16, Validation Loss: 1.5011, Validation Accuracy: 0.3399\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/16: 100%|██████████| 146/146 [04:09<00:00,  1.71s/batch, accuracy=0.365, loss=1.44]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.35      0.34       259\n",
            "           1       0.45      0.27      0.34       285\n",
            "           2       0.38      0.48      0.42       233\n",
            "           3       0.26      0.48      0.34       180\n",
            "           4       0.22      0.15      0.18       128\n",
            "           5       0.25      0.03      0.05        77\n",
            "\n",
            "    accuracy                           0.33      1162\n",
            "   macro avg       0.31      0.29      0.28      1162\n",
            "weighted avg       0.34      0.33      0.32      1162\n",
            "\n",
            "[[ 90  51  58  47  12   1]\n",
            " [ 88  77  70  42   8   0]\n",
            " [ 20  29 112  60  11   1]\n",
            " [ 25   7  46  86  13   3]\n",
            " [ 26   7   7  68  19   1]\n",
            " [ 23   2   2  23  25   2]]\n",
            "Epoch 15/16, Validation Loss: 1.5349, Validation Accuracy: 0.3322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/16: 100%|██████████| 146/146 [04:19<00:00,  1.78s/batch, accuracy=0.396, loss=1.39]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.31      0.31       259\n",
            "           1       0.44      0.53      0.48       285\n",
            "           2       0.51      0.36      0.43       233\n",
            "           3       0.31      0.32      0.32       180\n",
            "           4       0.29      0.37      0.32       128\n",
            "           5       0.38      0.29      0.33        77\n",
            "\n",
            "    accuracy                           0.38      1162\n",
            "   macro avg       0.38      0.36      0.36      1162\n",
            "weighted avg       0.39      0.38      0.38      1162\n",
            "\n",
            "[[ 79 101  30  21  16  12]\n",
            " [ 58 151  25  30  17   4]\n",
            " [ 23  58  85  44  22   1]\n",
            " [ 38  19  22  58  36   7]\n",
            " [ 30   7   4  28  47  12]\n",
            " [ 19   4   0   7  25  22]]\n",
            "Epoch 16/16, Validation Loss: 1.3914, Validation Accuracy: 0.3804\n",
            "Best model saved!\n",
            "Finished Training\n",
            "Best validation accuracy: 0.3804\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "if __name__ == \"__main__\":\n",
        "    model = TunnedBlockStackNet8()\n",
        "    trainer = ClassificationBlockStackTrainer1(\n",
        "        csv_file = './COMP90086_2024_Project_train/train.csv', ##\n",
        "        img_dir='./COMP90086_2024_Project_train/train', ##\n",
        "        model=model,\n",
        "        test_size=0.2, # used to control the size of data in split_dataset(self)\n",
        "        num_epochs=16,\n",
        "        batch_size=32\n",
        "        )\n",
        "    trainer.train()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
