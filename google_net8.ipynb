{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baseline models\n",
        "* Regression\n",
        "* CLassification\n",
        "    * BlockStackNet4() googlenet-(64,6) - highest validation score 0.4863"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HWaiSBwEkz8V"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7vkeZCssFSz"
      },
      "source": [
        "## Baseline Models is defined\n",
        "gogglenet\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k25H8iyjtQzN"
      },
      "source": [
        "## The data set file path is defined here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vwhixF45mZlT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "'''\n",
        "In this function block, the funciton is defined to do\n",
        "1. def__init__() : get the image from the trainning data path\n",
        "2. def __len__() : get the size of hte dataset\n",
        "3. def __getitem__() : get the ralated image the correct dataset\n",
        "'''\n",
        "class BlockStackDataset(Dataset):\n",
        "  '''\n",
        "  Return the image and its label. If not access or not exist, raise error.\n",
        "  '''\n",
        "  def __init__(self, data_frame, img_dir, transform = None):\n",
        "    self.data_frame = data_frame\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    if not os.path.exists(self.img_dir):\n",
        "      raise ValueError(f\"File path {self.img_dir} not exisits!\")\n",
        "    if not os.access(self.img_dir, os.R_OK):\n",
        "      raise ValueError(f\"File path {self.img_dir} is not readable!\")\n",
        "  '''\n",
        "  Return the size of the dataset\n",
        "  '''\n",
        "  def __len__(self):\n",
        "    return len(self.data_frame)\n",
        "\n",
        "  '''\n",
        "  Read the image and according to its id, get the related data in the train.csv.\n",
        "\n",
        "  Then return the corresponding image , stable_height, instability_type dataset provided.\n",
        "  '''\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = os.path.join(self.img_dir, str(self.data_frame.iloc[idx, 0])) # the first column in the train == image name\n",
        "    image = Image.open(img_name + \".jpg\")\n",
        "    label = self.data_frame.iloc[idx, -1] # stable_height\n",
        "    instability_type = self.data_frame.iloc[idx,4] # instability_type\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label, instability_type\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GACK9K_uPnV"
      },
      "source": [
        "## Trainner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xrOPfPa4t0AD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix,classification_report\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime as datatime\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NfTZbRKSxh2C"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The function here helps to create the logs recording the experiment results.\n",
        "'''\n",
        "def create_log_dir():\n",
        "  current_time = datatime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # log_dir = f'/content/drive/MyDrive/CV final project/runs/experiment_{current_time}'\n",
        "  # solution_dir = f'/content/drive/MyDrive/CV final project/trained_models/experiment_{current_time}'\n",
        "\n",
        "  log_dir = f'./runs/experiment_{current_time}'\n",
        "  solution_dir = f'./runs/experiment_{current_time}'\n",
        "  if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "  if not os.path.exists(solution_dir):\n",
        "    os.makedirs(solution_dir)\n",
        "\n",
        "  return log_dir, solution_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7ox5LZ7Aage0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define models: googlenet\n",
        "\n",
        "class TunnedBlockStackNet8(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TunnedBlockStackNet8, self).__init__()\n",
        "        # load the pre-trained model: gogglenet\n",
        "        self.googlenet = models.googlenet(weights = models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        num_ftrs = self.googlenet.fc.in_features\n",
        "        self.googlenet.fc = nn.Identity()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, 6)\n",
        "\n",
        "        )\n",
        "    def forward(self, x):\n",
        "      x = self.googlenet(x)\n",
        "      x = self.fc(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "R3ipvbupjF6v"
      },
      "outputs": [],
      "source": [
        "class ClassificationBlockStackTrainer1:\n",
        "  '''\n",
        "  The function here helps to initalize the parameters used in the models and pre-process the image\n",
        "  '''\n",
        "  def __init__(self, csv_file, img_dir, model, stratify_column='stable_height', test_size=0.2,\n",
        "                 batch_size=32, num_epochs=10, learning_rate=0.001 ,random_state=42):\n",
        "        self.csv_file = csv_file\n",
        "        self.img_dir = img_dir\n",
        "        self.stratify_column = stratify_column\n",
        "        self.test_size = test_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = model\n",
        "\n",
        "        # load the train dataset\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "\n",
        "        # split data into train and validation dataset\n",
        "        self.train_data, self.val_data, self.train_ids, self.valid_ids = self.split_dataset()\n",
        "\n",
        "        # pre-processing the images\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "\n",
        "              #ADD MORE TRANSFORM METHODS HERE\n",
        "\n",
        "            transforms.RandomHorizontalFlip(),## ADDDED\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)), ## ADDDED\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),## ADDDED\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.train_loader = self.create_dataloader(self.train_data, self.transform)\n",
        "        self.val_loader = self.create_dataloader(self.val_data, self.transform, shuffle=False)\n",
        "\n",
        "\n",
        "        # use the gpu\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # set the loss weight\n",
        "        '''\n",
        "        Changed some thing here : to see the results\n",
        "        1. add the class_weights to each class labels with different weights,\n",
        "           more lables occurece with lower weight, few label occurence with higher weight;\n",
        "           without the weights, all the labels will be treated equally.\n",
        "\n",
        "        2. used to apply L2 regularization (also called weight decay).\n",
        "        The primary purpose of weight decay is to prevent overfitting by penalizing large weights.\n",
        "         It adds a penalty to the loss function based on the size of the weights\n",
        "         helps it generalize better to unseen data.\n",
        "         Lnew = Lold + weight_decay * sum(weight^2)\n",
        "\n",
        "        3. add scheduler to Reduces the learning rate after every step_size epochs.\n",
        "          After every 4 epochs, the learning rate will be multiplied by gamma (0.1 here), reducing it by 90%.\n",
        "        '''\n",
        "        class_weights = torch.tensor([100/25 , 100/25, 100/20,  100/15, 100/10,  100/5], device= self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=class_weights) # CrossEntropy for multi categorical-label predication\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate , weight_decay=0.0001)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size = 15, gamma=0.1)\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function helps to split the data set into the training and validation dataset according to the\n",
        "  size pre-determined.\n",
        "  '''\n",
        "\n",
        "  def split_dataset(self):\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=self.test_size, random_state=42)\n",
        "    train_ids = []\n",
        "    valid_ids = []\n",
        "    for train_idx, val_idx in split.split(self.data_frame, self.data_frame[self.stratify_column]):\n",
        "      train_data = self.data_frame.iloc[train_idx]\n",
        "      val_data = self.data_frame.iloc[val_idx]\n",
        "      train_ids.append(train_idx)\n",
        "      valid_ids.append(val_idx)\n",
        "    print(f\"Train dataset size: {len(train_data)}\",\n",
        "       f\"Validation dataset size: {len(val_data)}\",\n",
        "       f\"length of train_ids{(len(train_ids))}\",\n",
        "       f\"length of valid_ids{(len(valid_ids))}\")\n",
        "    return train_data, val_data, train_ids, valid_ids\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function helps to loda the image\n",
        "  '''\n",
        "  def create_dataloader(self, data_frame, transform, shuffle=True):\n",
        "    dataset = BlockStackDataset(data_frame, self.img_dir, transform=transform) # transform 可以用来数据增强\n",
        "    return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "  def generate_classification_report(self, outputs, labels):\n",
        "    predicted = outputs # 'outputs' is already a numpy array after prediction\n",
        "    labels = labels\n",
        "    print(classification_report(labels, predicted, zero_division=0))\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_confusion_matrix(self, outputs, labels):\n",
        "    predicted = outputs\n",
        "    labels = labels\n",
        "    matrix = confusion_matrix(labels, predicted)\n",
        "    print(matrix)\n",
        "\n",
        "\n",
        "  def validate(self):\n",
        "    self.model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_labels = []  # add all the lables\n",
        "    all_predictions = []  # add all the prediction\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, _ in self.val_loader:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device).long()\n",
        "            labels = labels - 1\n",
        "            outputs = self.model(inputs)\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "            # collecting all the lables and predictions\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "            # calculate the loss\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # calculate the correct predication\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # calculate the accuracy rate\n",
        "    val_accuracy = correct_predictions / total_samples\n",
        "    self.generate_classification_report(np.array(all_predictions), np.array(all_labels))\n",
        "    self.calculate_confusion_matrix(np.array(all_predictions), np.array(all_labels))\n",
        "    return val_loss/len(self.val_loader),val_accuracy, all_labels, all_predictions\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function here is used as the main training function on the image by using the pre-definned models in\n",
        "  hte first model class.\n",
        "  '''\n",
        "\n",
        "  def train(self):\n",
        "      _, solution_dir = create_log_dir()\n",
        "\n",
        "      best_val_accuracy = 0.0\n",
        "      for epoch in range(self.num_epochs):\n",
        "          self.model.train()\n",
        "          running_loss = 0.0\n",
        "          running_accuracy = 0.0\n",
        "\n",
        "          # monitor the process\n",
        "          with tqdm(self.train_loader, unit=\"batch\") as tepoch:\n",
        "              tepoch.set_description(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "              for inputs, labels, _ in tepoch:\n",
        "                  inputs, labels = inputs.to(self.device), labels.to(self.device).long()\n",
        "                  labels = labels - 1\n",
        "\n",
        "                  #forward propagation\n",
        "                  self.optimizer.zero_grad()\n",
        "                  raw_outputs = self.model(inputs)\n",
        "                  loss = self.criterion(raw_outputs, labels)  # loss calculation\n",
        "\n",
        "                  loss.backward()  # backward propagation\n",
        "                  self.optimizer.step()\n",
        "\n",
        "                  #Loss calculating\n",
        "                  running_loss += loss.item()\n",
        "                  _, predicted = torch.max(raw_outputs, 1)\n",
        "                  accuracy = (predicted == labels).sum().item()/ labels.size(0)\n",
        "                  running_accuracy += accuracy\n",
        "                  tepoch.set_postfix(loss=running_loss / len(self.train_loader),\n",
        "                            accuracy=running_accuracy / len(self.train_loader))\n",
        "\n",
        "\n",
        "\n",
        "          self.scheduler.step()\n",
        "          print(self.scheduler.get_last_lr())\n",
        "\n",
        "\n",
        "          val_loss,val_accuracy, all_labels, all_predictions = self.validate()\n",
        "          print(f\"Epoch {epoch + 1}/{self.num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "          if val_accuracy > best_val_accuracy:\n",
        "              best_val_accuracy = val_accuracy\n",
        "              torch.save(self.model.state_dict(), f'{solution_dir}/best_model.pth')\n",
        "              print('Best model saved!')\n",
        "\n",
        "      print('Finished Training')\n",
        "      print(f'Best validation accuracy: {best_val_accuracy:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "cannot assign module before Module.__init__() call",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m TunnedBlockStackNet8()\n\u001b[0;32m----> 4\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m ClassificationBlockStackTrainer1(\n\u001b[1;32m      5\u001b[0m         csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./COMP90086_2024_Project_train/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         img_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./COMP90086_2024_Project_train/train\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      8\u001b[0m         test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;66;03m# used to control the size of data in split_dataset(self)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     11\u001b[0m         )\n\u001b[1;32m     12\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "Cell \u001b[0;32mIn[38], line 14\u001b[0m, in \u001b[0;36mClassificationBlockStackTrainer1.__init__\u001b[0;34m(self, csv_file, img_dir, model, stratify_column, test_size, batch_size, num_epochs, learning_rate, random_state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;241m=\u001b[39m num_epochs\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# load the train dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1757\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1757\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1758\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1759\u001b[0m     remove_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set)\n\u001b[1;32m   1760\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
            "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
          ]
        }
      ],
      "source": [
        "# test\n",
        "if __name__ == \"__main__\":\n",
        "    model = TunnedBlockStackNet8()\n",
        "    trainer = ClassificationBlockStackTrainer1(\n",
        "        csv_file = './COMP90086_2024_Project_train/train.csv', ##\n",
        "        img_dir='./COMP90086_2024_Project_train/train', ##\n",
        "        model=model,\n",
        "        test_size=0.2, # used to control the size of data in split_dataset(self)\n",
        "        num_epochs=1,\n",
        "        batch_size=32\n",
        "        )\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ids = trainer.train_ids\n",
        "valid_ids = trainer.valid_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'valid_ids' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(valid_ids[\u001b[38;5;241m0\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'valid_ids' is not defined"
          ]
        }
      ],
      "source": [
        "len(valid_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.70      0.72       384\n",
            "           1       0.70      0.70      0.70       384\n",
            "           2       0.61      0.68      0.64       307\n",
            "           3       0.58      0.62      0.60       230\n",
            "           4       0.53      0.49      0.51       154\n",
            "           5       0.43      0.35      0.39        77\n",
            "\n",
            "    accuracy                           0.64      1536\n",
            "   macro avg       0.60      0.59      0.59      1536\n",
            "weighted avg       0.64      0.64      0.64      1536\n",
            "\n",
            "[[267  41  32  26  11   7]\n",
            " [ 34 269  38  27  12   4]\n",
            " [ 20  35 209  23  12   8]\n",
            " [ 19  13  31 143  18   6]\n",
            " [ 14  10  20  24  75  11]\n",
            " [  7  14  13   3  13  27]]\n"
          ]
        }
      ],
      "source": [
        "val_loss,val_accuracy, all_labels, all_predictions = trainer.validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.64453125"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comparison = map(lambda all_labels, all_predictions: all_labels == all_predictions, all_labels, all_predictions)\n",
        "sum(comparison)/len(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>shapeset</th>\n",
              "      <th>type</th>\n",
              "      <th>total_height</th>\n",
              "      <th>instability_type</th>\n",
              "      <th>cam_angle</th>\n",
              "      <th>stable_height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7334</th>\n",
              "      <td>956915</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3955</th>\n",
              "      <td>516709</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>77447</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>212770</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5645</th>\n",
              "      <td>745098</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  shapeset  type  total_height  instability_type  cam_angle  \\\n",
              "7334  956915         2     2             4                 2          1   \n",
              "3955  516709         1     2             5                 1          1   \n",
              "619    77447         1     2             5                 1          1   \n",
              "1594  212770         2     2             4                 2          1   \n",
              "5645  745098         2     2             6                 1          1   \n",
              "\n",
              "      stable_height  \n",
              "7334              1  \n",
              "3955              3  \n",
              "619               3  \n",
              "1594              1  \n",
              "5645              4  "
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data = trainer.val_data\n",
        "val_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_39098/1482792748.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_data['predicted'] = [pred+1 for pred in all_predictions]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>shapeset</th>\n",
              "      <th>type</th>\n",
              "      <th>total_height</th>\n",
              "      <th>instability_type</th>\n",
              "      <th>cam_angle</th>\n",
              "      <th>stable_height</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7334</th>\n",
              "      <td>956915</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3955</th>\n",
              "      <td>516709</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>77447</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>212770</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5645</th>\n",
              "      <td>745098</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  shapeset  type  total_height  instability_type  cam_angle  \\\n",
              "7334  956915         2     2             4                 2          1   \n",
              "3955  516709         1     2             5                 1          1   \n",
              "619    77447         1     2             5                 1          1   \n",
              "1594  212770         2     2             4                 2          1   \n",
              "5645  745098         2     2             6                 1          1   \n",
              "\n",
              "      stable_height  predicted  \n",
              "7334              1          1  \n",
              "3955              3          2  \n",
              "619               3          3  \n",
              "1594              1          1  \n",
              "5645              4          2  "
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data['predicted'] = [pred+1 for pred in all_predictions]\n",
        "val_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_39098/2673748297.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_data['pred_type'] = val_data['predicted'] == val_data['stable_height']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>shapeset</th>\n",
              "      <th>type</th>\n",
              "      <th>total_height</th>\n",
              "      <th>instability_type</th>\n",
              "      <th>cam_angle</th>\n",
              "      <th>stable_height</th>\n",
              "      <th>predicted</th>\n",
              "      <th>pred_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7334</th>\n",
              "      <td>956915</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3955</th>\n",
              "      <td>516709</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>77447</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>212770</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5645</th>\n",
              "      <td>745098</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  shapeset  type  total_height  instability_type  cam_angle  \\\n",
              "7334  956915         2     2             4                 2          1   \n",
              "3955  516709         1     2             5                 1          1   \n",
              "619    77447         1     2             5                 1          1   \n",
              "1594  212770         2     2             4                 2          1   \n",
              "5645  745098         2     2             6                 1          1   \n",
              "\n",
              "      stable_height  predicted  pred_type  \n",
              "7334              1          1       True  \n",
              "3955              3          2      False  \n",
              "619               3          3       True  \n",
              "1594              1          1       True  \n",
              "5645              4          2      False  "
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data['pred_type'] = val_data['predicted'] == val_data['stable_height']\n",
        "val_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.64453125"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_data[val_data['predicted']== val_data['stable_height']])/len(valid_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.64453125"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data['pred_type'].sum()/len(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "instability_type\n",
              "0    369\n",
              "1    777\n",
              "2    390\n",
              "Name: pred_type, dtype: int64"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.groupby('instability_type').pred_type.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "shapeset\n",
              "1    0.616105\n",
              "2    0.659681\n",
              "Name: pred_type, dtype: float64"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.groupby('shapeset').pred_type.sum()/val_data.groupby('shapeset').pred_type.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "type\n",
              "1    0.768021\n",
              "2    0.522639\n",
              "Name: pred_type, dtype: float64"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.groupby('type').pred_type.sum()/val_data.groupby('type').pred_type.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "instability_type\n",
              "0    0.544715\n",
              "1    0.550837\n",
              "2    0.925641\n",
              "Name: pred_type, dtype: float64"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.groupby('instability_type').pred_type.sum()/val_data.groupby('instability_type').pred_type.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>shapeset</th>\n",
              "      <th>type</th>\n",
              "      <th>total_height</th>\n",
              "      <th>instability_type</th>\n",
              "      <th>cam_angle</th>\n",
              "      <th>stable_height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>173</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>245</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>465</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>611</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id  shapeset  type  total_height  instability_type  cam_angle  \\\n",
              "0   54         2     1             3                 1          1   \n",
              "1  173         1     1             4                 1          2   \n",
              "2  245         1     1             4                 1          2   \n",
              "3  465         2     1             5                 0          1   \n",
              "4  611         2     1             3                 1          1   \n",
              "\n",
              "   stable_height  \n",
              "0              2  \n",
              "1              1  \n",
              "2              1  \n",
              "3              5  \n",
              "4              1  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./COMP90086_2024_Project_train/train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>shapeset</th>\n",
              "      <th>type</th>\n",
              "      <th>total_height</th>\n",
              "      <th>cam_angle</th>\n",
              "      <th>stable_height</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>instability_type</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3840</td>\n",
              "      <td>3840</td>\n",
              "      <td>3840</td>\n",
              "      <td>3840</td>\n",
              "      <td>3840</td>\n",
              "      <td>3840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    id  shapeset  type  total_height  cam_angle  stable_height\n",
              "instability_type                                                              \n",
              "0                 1920      1920  1920          1920       1920           1920\n",
              "1                 3840      3840  3840          3840       3840           3840\n",
              "2                 1920      1920  1920          1920       1920           1920"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby('instability_type').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_67928/1284981419.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model = torch.load('./runs/experiment_20241012-010838/best_model.pth')\n"
          ]
        }
      ],
      "source": [
        "best_model = torch.load('./runs/experiment_20241012-010838/best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_67928/1886415100.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model = torch.load('./runs/experiment_20241012-010838/best_model.pth',map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./runs/experiment_20241012-010838/best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m best_model\u001b[38;5;241m.\u001b[39meval()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
          ]
        }
      ],
      "source": [
        "best_model = torch.load('./runs/experiment_20241012-010838/best_model.pth',map_location=torch.device('cpu'))\n",
        "best_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoadModel(nn.Module):\n",
        "  '''\n",
        "  The function here helps to initalize the parameters used in the models and pre-process the image\n",
        "  '''\n",
        "  def __init__(self, csv_file, img_dir, model, stratify_column='stable_height', test_size=0.2,\n",
        "                 batch_size=32, num_epochs=10, learning_rate=0.001 ,random_state=42):\n",
        "        self.csv_file = csv_file\n",
        "        self.img_dir = img_dir\n",
        "        self.stratify_column = stratify_column\n",
        "        self.test_size = test_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = model\n",
        "\n",
        "        # load the train dataset\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "\n",
        "        # split data into train and validation dataset\n",
        "        self.train_data, self.val_data, self.train_ids, self.valid_ids = self.split_dataset()\n",
        "\n",
        "        # pre-processing the images\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "\n",
        "              #ADD MORE TRANSFORM METHODS HERE\n",
        "\n",
        "            transforms.RandomHorizontalFlip(),## ADDDED\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)), ## ADDDED\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),## ADDDED\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.train_loader = self.create_dataloader(self.train_data, self.transform)\n",
        "        self.val_loader = self.create_dataloader(self.val_data, self.transform, shuffle=False)\n",
        "\n",
        "\n",
        "        # use the gpu\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # set the loss weight\n",
        "        '''\n",
        "        Changed some thing here : to see the results\n",
        "        1. add the class_weights to each class labels with different weights,\n",
        "           more lables occurece with lower weight, few label occurence with higher weight;\n",
        "           without the weights, all the labels will be treated equally.\n",
        "\n",
        "        2. used to apply L2 regularization (also called weight decay).\n",
        "        The primary purpose of weight decay is to prevent overfitting by penalizing large weights.\n",
        "         It adds a penalty to the loss function based on the size of the weights\n",
        "         helps it generalize better to unseen data.\n",
        "         Lnew = Lold + weight_decay * sum(weight^2)\n",
        "\n",
        "        3. add scheduler to Reduces the learning rate after every step_size epochs.\n",
        "          After every 4 epochs, the learning rate will be multiplied by gamma (0.1 here), reducing it by 90%.\n",
        "        '''\n",
        "        class_weights = torch.tensor([100/25 , 100/25, 100/20,  100/15, 100/10,  100/5], device= self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=class_weights) # CrossEntropy for multi categorical-label predication\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate , weight_decay=0.0001)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size = 15, gamma=0.1)\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function helps to split the data set into the training and validation dataset according to the\n",
        "  size pre-determined.\n",
        "  '''\n",
        "\n",
        "  def split_dataset(self):\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=self.test_size, random_state=42)\n",
        "    train_ids = []\n",
        "    valid_ids = []\n",
        "    for train_idx, val_idx in split.split(self.data_frame, self.data_frame[self.stratify_column]):\n",
        "      train_data = self.data_frame.iloc[train_idx]\n",
        "      val_data = self.data_frame.iloc[val_idx]\n",
        "      train_ids.append(train_idx)\n",
        "      valid_ids.append(val_idx)\n",
        "    print(f\"Train dataset size: {len(train_data)}\",\n",
        "       f\"Validation dataset size: {len(val_data)}\",\n",
        "       f\"length of train_ids{(len(train_ids))}\",\n",
        "       f\"length of valid_ids{(len(valid_ids))}\")\n",
        "    return train_data, val_data, train_ids, valid_ids\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function helps to loda the image\n",
        "  '''\n",
        "  def create_dataloader(self, data_frame, transform, shuffle=True):\n",
        "    dataset = BlockStackDataset(data_frame, self.img_dir, transform=transform) # transform 可以用来数据增强\n",
        "    return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "  def generate_classification_report(self, outputs, labels):\n",
        "    predicted = outputs # 'outputs' is already a numpy array after prediction\n",
        "    labels = labels\n",
        "    print(classification_report(labels, predicted, zero_division=0))\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_confusion_matrix(self, outputs, labels):\n",
        "    predicted = outputs\n",
        "    labels = labels\n",
        "    matrix = confusion_matrix(labels, predicted)\n",
        "    print(matrix)\n",
        "\n",
        "\n",
        "  def validate(self):\n",
        "    self.model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_labels = []  # add all the lables\n",
        "    all_predictions = []  # add all the prediction\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, _ in self.val_loader:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device).long()\n",
        "            labels = labels - 1\n",
        "            outputs = self.model(inputs)\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "            # collecting all the lables and predictions\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "            # calculate the loss\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # calculate the correct predication\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # calculate the accuracy rate\n",
        "    val_accuracy = correct_predictions / total_samples\n",
        "    self.generate_classification_report(np.array(all_predictions), np.array(all_labels))\n",
        "    self.calculate_confusion_matrix(np.array(all_predictions), np.array(all_labels))\n",
        "    return val_loss/len(self.val_loader),val_accuracy, all_labels, all_predictions\n",
        "\n",
        "\n",
        "  '''\n",
        "  The function here is used as the main training function on the image by using the pre-definned models in\n",
        "  hte first model class.\n",
        "  '''\n",
        "\n",
        "  def train(self):\n",
        "      _, solution_dir = create_log_dir()\n",
        "\n",
        "      best_val_accuracy = 0.0\n",
        "      for epoch in range(self.num_epochs):\n",
        "          self.model.train()\n",
        "          running_loss = 0.0\n",
        "          running_accuracy = 0.0\n",
        "\n",
        "          # monitor the process\n",
        "          with tqdm(self.train_loader, unit=\"batch\") as tepoch:\n",
        "              tepoch.set_description(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "              for inputs, labels, _ in tepoch:\n",
        "                  inputs, labels = inputs.to(self.device), labels.to(self.device).long()\n",
        "                  labels = labels - 1\n",
        "\n",
        "                  #forward propagation\n",
        "                  self.optimizer.zero_grad()\n",
        "                  raw_outputs = self.model(inputs)\n",
        "                  loss = self.criterion(raw_outputs, labels)  # loss calculation\n",
        "\n",
        "                  loss.backward()  # backward propagation\n",
        "                  self.optimizer.step()\n",
        "\n",
        "                  #Loss calculating\n",
        "                  running_loss += loss.item()\n",
        "                  _, predicted = torch.max(raw_outputs, 1)\n",
        "                  accuracy = (predicted == labels).sum().item()/ labels.size(0)\n",
        "                  running_accuracy += accuracy\n",
        "                  tepoch.set_postfix(loss=running_loss / len(self.train_loader),\n",
        "                            accuracy=running_accuracy / len(self.train_loader))\n",
        "\n",
        "\n",
        "\n",
        "          self.scheduler.step()\n",
        "          print(self.scheduler.get_last_lr())\n",
        "\n",
        "\n",
        "          val_loss,val_accuracy, all_labels, all_predictions = self.validate()\n",
        "          print(f\"Epoch {epoch + 1}/{self.num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "          if val_accuracy > best_val_accuracy:\n",
        "              best_val_accuracy = val_accuracy\n",
        "              torch.save(self.model.state_dict(), f'{solution_dir}/best_model.pth')\n",
        "              print('Best model saved!')\n",
        "\n",
        "      print('Finished Training')\n",
        "      print(f'Best validation accuracy: {best_val_accuracy:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "cannot assign module before Module.__init__() call",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TunnedBlockStackNet8()\n\u001b[0;32m----> 3\u001b[0m bm \u001b[38;5;241m=\u001b[39m LoadModel(\n\u001b[1;32m      4\u001b[0m     csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./COMP90086_2024_Project_train/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         img_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./COMP90086_2024_Project_train/train\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m         test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;66;03m# used to control the size of data in split_dataset(self)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      9\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
            "Cell \u001b[0;32mIn[36], line 14\u001b[0m, in \u001b[0;36mLoadModel.__init__\u001b[0;34m(self, csv_file, img_dir, model, stratify_column, test_size, batch_size, num_epochs, learning_rate, random_state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;241m=\u001b[39m num_epochs\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# load the train dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1757\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1757\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1758\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1759\u001b[0m     remove_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set)\n\u001b[1;32m   1760\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
            "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
          ]
        }
      ],
      "source": [
        "model = TunnedBlockStackNet8()\n",
        "\n",
        "bm = LoadModel(\n",
        "    csv_file = './COMP90086_2024_Project_train/train.csv', ##\n",
        "        img_dir='./COMP90086_2024_Project_train/train', ##\n",
        "        model=model,\n",
        "        test_size=0.2, # used to control the size of data in split_dataset(self)\n",
        "        num_epochs=30,\n",
        "        batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LoadModel' object has no attribute 'to'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 指定设备（GPU 或 CPU）\u001b[39;00m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m bm\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 加载保存的 state_dict\u001b[39;00m\n\u001b[1;32m      6\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./runs/experiment_20241012-010838/best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LoadModel' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "# 指定设备（GPU 或 CPU）\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bm.to(device)\n",
        "\n",
        "# 加载保存的 state_dict\n",
        "state_dict = torch.load('./runs/experiment_20241012-010838/best_model.pth', map_location=device)\n",
        "\n",
        "# 将 state_dict 加载到模型\n",
        "bm.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Model\n",
        "\n",
        "Net8, epoch = 8, no batch_size defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q39AMlmH_3fn",
        "outputId": "f009ffad-9b8d-4a78-eaa4-cb942caa4ba2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /Users/macbookpro/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n",
            "100%|██████████| 49.7M/49.7M [00:03<00:00, 14.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 6144 Validation dataset size: 1536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/8: 100%|██████████| 192/192 [06:38<00:00,  2.07s/batch, accuracy=0.274, loss=1.58]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.42      0.45       384\n",
            "           1       0.38      0.45      0.41       384\n",
            "           2       0.30      0.62      0.41       307\n",
            "           3       0.25      0.06      0.09       230\n",
            "           4       0.26      0.12      0.16       154\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.36      1536\n",
            "   macro avg       0.28      0.28      0.25      1536\n",
            "weighted avg       0.34      0.36      0.33      1536\n",
            "\n",
            "[[161 134  78   4   7   0]\n",
            " [ 70 172 132   4   6   0]\n",
            " [ 26  71 191  12   7   0]\n",
            " [ 28  47 124  13  18   0]\n",
            " [ 32  12  76  16  18   0]\n",
            " [ 13  11  35   4  14   0]]\n",
            "Epoch 1/8, Validation Loss: 1.5266, Validation Accuracy: 0.3613\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/8: 100%|██████████| 192/192 [06:44<00:00,  2.11s/batch, accuracy=0.363, loss=1.45]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.63      0.51       384\n",
            "           1       0.81      0.17      0.28       384\n",
            "           2       0.42      0.39      0.40       307\n",
            "           3       0.33      0.28      0.30       230\n",
            "           4       0.22      0.57      0.31       154\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.38      1536\n",
            "   macro avg       0.37      0.34      0.30      1536\n",
            "weighted avg       0.46      0.38      0.35      1536\n",
            "\n",
            "[[242  10  35  32  65   0]\n",
            " [145  64  79  29  67   0]\n",
            " [ 91   5 119  35  57   0]\n",
            " [ 53   0  44  65  68   0]\n",
            " [ 28   0   5  33  88   0]\n",
            " [  6   0   3   5  63   0]]\n",
            "Epoch 2/8, Validation Loss: 1.4068, Validation Accuracy: 0.3763\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/8: 100%|██████████| 192/192 [06:37<00:00,  2.07s/batch, accuracy=0.417, loss=1.38]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.39      0.49       384\n",
            "           1       0.60      0.41      0.49       384\n",
            "           2       0.41      0.61      0.49       307\n",
            "           3       0.38      0.45      0.41       230\n",
            "           4       0.29      0.56      0.38       154\n",
            "           5       0.29      0.03      0.05        77\n",
            "\n",
            "    accuracy                           0.45      1536\n",
            "   macro avg       0.44      0.41      0.38      1536\n",
            "weighted avg       0.50      0.45      0.44      1536\n",
            "\n",
            "[[151  64  77  48  44   0]\n",
            " [ 36 157 112  37  40   2]\n",
            " [ 12  22 188  50  34   1]\n",
            " [ 11   7  59 104  48   1]\n",
            " [ 11   6  19  30  87   1]\n",
            " [  9   4   5   7  50   2]]\n",
            "Epoch 3/8, Validation Loss: 1.3109, Validation Accuracy: 0.4486\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/8: 100%|██████████| 192/192 [06:59<00:00,  2.18s/batch, accuracy=0.437, loss=1.33]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.67      0.54       384\n",
            "           1       0.51      0.55      0.53       384\n",
            "           2       0.57      0.38      0.45       307\n",
            "           3       0.48      0.34      0.40       230\n",
            "           4       0.31      0.35      0.33       154\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.47      1536\n",
            "   macro avg       0.39      0.38      0.38      1536\n",
            "weighted avg       0.46      0.47      0.45      1536\n",
            "\n",
            "[[259  80  19  14  11   1]\n",
            " [103 213  39  10  18   1]\n",
            " [ 67  65 116  35  23   1]\n",
            " [ 72  20  28  78  32   0]\n",
            " [ 52  23   2  23  54   0]\n",
            " [ 24  14   1   4  34   0]]\n",
            "Epoch 4/8, Validation Loss: 1.2569, Validation Accuracy: 0.4688\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/8: 100%|██████████| 192/192 [06:57<00:00,  2.17s/batch, accuracy=0.461, loss=1.3]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.74      0.61       384\n",
            "           1       0.68      0.37      0.48       384\n",
            "           2       0.46      0.52      0.49       307\n",
            "           3       0.45      0.45      0.45       230\n",
            "           4       0.32      0.45      0.38       154\n",
            "           5       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.49      1536\n",
            "   macro avg       0.41      0.42      0.40      1536\n",
            "weighted avg       0.50      0.49      0.48      1536\n",
            "\n",
            "[[283  34  36  12  19   0]\n",
            " [ 97 143  91  24  29   0]\n",
            " [ 69  12 159  43  24   0]\n",
            " [ 46   6  37 103  38   0]\n",
            " [ 30   5  13  36  70   0]\n",
            " [ 16   9   6   9  37   0]]\n",
            "Epoch 5/8, Validation Loss: 1.2357, Validation Accuracy: 0.4935\n",
            "Best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/8: 100%|██████████| 192/192 [06:43<00:00,  2.10s/batch, accuracy=0.487, loss=1.26]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.38      0.51       384\n",
            "           1       0.69      0.51      0.58       384\n",
            "           2       0.53      0.53      0.53       307\n",
            "           3       0.42      0.47      0.44       230\n",
            "           4       0.25      0.69      0.37       154\n",
            "           5       0.12      0.12      0.12        77\n",
            "\n",
            "    accuracy                           0.47      1536\n",
            "   macro avg       0.46      0.45      0.43      1536\n",
            "weighted avg       0.56      0.47      0.49      1536\n",
            "\n",
            "[[147  52  53  45  80   7]\n",
            " [ 23 194  65  32  46  24]\n",
            " [  6  17 162  56  51  15]\n",
            " [  9   5  19 109  84   4]\n",
            " [  6   6   6  17 106  13]\n",
            " [  1   8   0   1  58   9]]\n",
            "Epoch 6/8, Validation Loss: 1.3388, Validation Accuracy: 0.4733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/8: 100%|██████████| 192/192 [06:46<00:00,  2.12s/batch, accuracy=0.49, loss=1.22]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.70      0.54       384\n",
            "           1       0.51      0.58      0.54       384\n",
            "           2       0.48      0.23      0.31       307\n",
            "           3       0.42      0.33      0.37       230\n",
            "           4       0.40      0.38      0.39       154\n",
            "           5       0.31      0.05      0.09        77\n",
            "\n",
            "    accuracy                           0.46      1536\n",
            "   macro avg       0.43      0.38      0.37      1536\n",
            "weighted avg       0.45      0.46      0.43      1536\n",
            "\n",
            "[[268  78  14  18   6   0]\n",
            " [103 222  22  20  15   2]\n",
            " [ 72 101  71  47  14   2]\n",
            " [ 66  20  33  76  35   0]\n",
            " [ 63   7   5  15  59   5]\n",
            " [ 38   9   4   3  19   4]]\n",
            "Epoch 7/8, Validation Loss: 1.3688, Validation Accuracy: 0.4557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/8: 100%|██████████| 192/192 [06:54<00:00,  2.16s/batch, accuracy=0.515, loss=1.2]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.001]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.58      0.61       384\n",
            "           1       0.60      0.60      0.60       384\n",
            "           2       0.48      0.57      0.52       307\n",
            "           3       0.47      0.50      0.49       230\n",
            "           4       0.36      0.36      0.36       154\n",
            "           5       0.26      0.13      0.17        77\n",
            "\n",
            "    accuracy                           0.53      1536\n",
            "   macro avg       0.47      0.46      0.46      1536\n",
            "weighted avg       0.52      0.53      0.52      1536\n",
            "\n",
            "[[223  80  40  29  10   2]\n",
            " [ 47 231  71  18  14   3]\n",
            " [ 21  49 174  41  20   2]\n",
            " [ 24   8  48 116  28   6]\n",
            " [ 23   7  17  36  55  16]\n",
            " [ 12  10  13   7  25  10]]\n",
            "Epoch 8/8, Validation Loss: 1.1596, Validation Accuracy: 0.5267\n",
            "Best model saved!\n",
            "Finished Training\n",
            "Best validation accuracy: 0.5267\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model = TunnedBlockStackNet8()\n",
        "    trainer = ClassificationBlockStackTrainer1(\n",
        "        # csv_file = '/content/drive/MyDrive/CV final project/train data/train.csv', ##\n",
        "        # img_dir='/content/drive/MyDrive/CV final project/train data/train', ##\n",
        "        csv_file = './COMP90086_2024_Project_train/train.csv', ##\n",
        "        img_dir='./COMP90086_2024_Project_train/train', ##\n",
        "        model=model,\n",
        "        test_size=0.2, # used to control the size of data in split_dataset(self)\n",
        "        num_epochs=8,\n",
        "        #batch_size=32\n",
        "        )\n",
        "    trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
